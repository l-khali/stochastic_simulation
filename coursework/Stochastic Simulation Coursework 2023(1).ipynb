{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Simulation - Coursework 2023\n",
    "\n",
    "This assignment has two parts and graded over **100 marks**. Some general remarks:\n",
    "\n",
    "- The assignment is due on **11 December 2023, 1PM GMT**, to be submitted via Blackboard (see the instructions on the course website).\n",
    "- You should use this .ipynb file as a skeleton and you should submit a PDF report. Prepare the IPython notebook and export it as a PDF. If you can't export your notebook as PDF, then you can export it as HTML and then use the \"Print\" feature in browser (Chrome: File -> Print) and choose \"Save as PDF\". \n",
    "- Your PDF should be no longer than 20 pages. But please be concise.\n",
    "- You can reuse the code from the course material but note that this coursework also requires novel implementations. Try to personalise your code in order to avoid having problems with plagiarism checks. You can use Python's functions for sampling random variables of all distributions of your choice.\n",
    "- **Please comment your code properly.**\n",
    "\n",
    "Let us start our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(36) # You can change this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Model Selection via Perfect Monte Carlo (40 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following probabilistic model\n",
    "\\begin{align*}\n",
    "    p(x) &= \\mathcal{N}(x; 5, 0.01), \\\\\n",
    "    p(y_i|x) &= \\mathcal{N}(y_i; \\theta x, 0.05),\n",
    "\\end{align*}\n",
    "for $i = 1, \\ldots, T$ where $y_i$ are conditionally independent given $x$. You are given a dataset (see it on Blackboard) denoted here as $y_{1:T}$ for $T = 100$. As defined in the course, we can find the marginal likelihood as\n",
    "\\begin{align*}\n",
    "p_\\theta(y_{1:T}) = \\int p_\\theta(y_{1:T}|x) p(x) \\mathrm{d} x,\n",
    "\\end{align*}\n",
    "where we have left $\\theta$-dependence in the notation to emphasise that the marginal likelihood is a function of $\\theta$.\n",
    "\n",
    "Given the samples from prior $p(x)$, we can identify our test function above as $\\varphi(x) = p_\\theta(y_{1:T}|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** The first step is to write a log-likelihood function of $y_{1:T}$, i.e., $p_\\theta(y_{1:T} | x)$. Note that, this is the joint likelihood of conditionally i.i.d. observations $y_i$ given $x$. This function should take input the data set vector `y` as loaded from `y_perfect_mc.txt` below, $\\theta$ (scalar), and $x$ (scalar), and `sig` (likelihood variance which is given as 0.05 in the question but leave it as a variable) values to evaluate the log-likelihood. Note that log-likelihood will be a **sum** in this case, over individual log-likelihoods. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the dataset in the same folder as this notebook\n",
    "# the following line loads y_perfect_mc.txt\n",
    "y = np.loadtxt('y_perfect_mc.txt')\n",
    "y = np.array(y, dtype=np.float64)\n",
    "\n",
    "# fill in your function below.\n",
    "\n",
    "def log_likelihood(y, x, theta, sig): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "# uncomment and evaluate your likelihood (do not remove)\n",
    "# print(log_likelihood(y, 1, 1, 1))\n",
    "# print(log_likelihood(y, 1, 1, 0.1))\n",
    "# print(log_likelihood(y, -1, 2, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii)** Write a logsumexp function. Let $\\mathbf{v}$ be a vector of log-quantities and assume we need to compute $\\log \\sum_{i=1}^N \\exp(v_i)$ where $\\mathbf{v} = (v_1, \\ldots, v_N)$. This function is given as\n",
    "\\begin{align*}\n",
    "\\log \\sum_{i=1}^N \\exp(v_i) = \\log \\sum_{i=1}^N \\exp(v_i - v_{\\max}) + v_{\\max},\n",
    "\\end{align*}\n",
    "where $v_{\\max} = \\max_{i = 1,\\ldots,N} v_i$. Implement this as a function which takes a vector of log-values and returns the log of the sum of exponentials of the input values. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(v):\n",
    "    # v is a vector\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "\n",
    "# uncomment and evaluate your logsumexp function (do not remove)\n",
    "# print(logsumexp(np.array([1, 2, 3])))\n",
    "# print(logsumexp(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])))\n",
    "# print(logsumexp(np.array([5, 6, 9, 12])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(iii)** Now we are at the stage of implementing the log marginal likelihood estimator. Inspect your estimator as described in Part (i). Its particular form is not implementable without using the trick you have coded in Part (iii). Now, implement a function that returns the **log** of the MC estimator you derived in Part (i). This function will take in\n",
    "\n",
    "- `y` dataset vector\n",
    "- $\\theta$ parameter (scalar)\n",
    "- `x_samples` (`np.array` vector) which are $N$ Monte Carlo samples.\n",
    "- a variance (scalar) variable `sig` for the joint log likelihood $p_\\theta(y_{1:T} | x)$ that will be used in `log_likelihood` function (we will set this to 0.05 as given in the question).\n",
    "\n",
    "**Hint:** Notice that the log of the MC estimator of the marginal likelihood takes the form\n",
    "\\begin{align*}\n",
    "\\log \\frac{1}{N} \\sum_{i=1}^N p_\\theta(y_{1:T} | x^{(i)}),\n",
    "\\end{align*}\n",
    "as given in the question. You have to use $p_\\theta(y_{1:T} | x^{(i)}) = \\exp(\\log p_\\theta(y_{1:T} | x^{(i)}))$ to get a `logsumexp` structure, i.e., $\\log$ and $\\text{sum}$ (over particles) and $\\exp$ of $\\log p_\\theta(y_{1:T} | x^{(i)})$ where $i = 1, \\ldots, N$ and $x^{(i)}$ are the $N$ Monte Carlo samples (do **not** forget $1/N$ term too). Therefore, now use the function of Part (i) to compute $\\log p_\\theta(y_{1:T} | x^{(i)})$ for every $i = 1,\\ldots, N$ and Part (ii) `logsumexp` these values to compute the estimate of log marginal likelihood. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_marginal_likelihood(y, theta, x_samples, sig): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "# uncomment and evaluate your marginal likelihood (do not remove)\n",
    "\n",
    "# print(log_marginal_likelihood(y, 1, np.array([-1, 1]), 1))\n",
    "# print(log_marginal_likelihood(y, 1, np.array([-1, 1]), 0.1))\n",
    "# print(log_marginal_likelihood(y, 2, np.array([-1, 1]), 0.5))\n",
    "\n",
    "# note that the above test code takes 2 dimensional array instead of N particles for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(iv)** We will now try to find the most likely $\\theta$. For this part, you will run your `log_marginal_likelihood` function for a range of $\\theta$ values. Note that, for every $\\theta$ value, you need to sample $N$ new samples from the prior (do not reuse the same samples). Compute your estimator of the $\\log \\hat{\\pi}_{\\text{MC}}^N \\approx \\log p_\\theta(y_{1:T})$ for $\\theta$-range given below. Plot the log marginal likelihood estimator as a function of $\\theta$. **<span style=\"color:blue\">(5 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.05\n",
    "sig_prior = 0.01\n",
    "mu_prior = 5.0\n",
    "\n",
    "N = 1000\n",
    "\n",
    "theta_range = np.linspace(0, 10, 500)\n",
    "log_ml_list = np.array([]) # you can use np.append to add elements to this array\n",
    "\n",
    "# fill in your code here\n",
    "\n",
    "# uncomment and plot your results (do not remove)\n",
    "\n",
    "# plt.plot(theta_range, log_ml_list)\n",
    "# plt.xlabel(r'$\\theta$')\n",
    "# plt.ylabel(r'$\\log p_\\theta(y_{1:T})$')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(v)** Now you have `log_ml_list` variable that corresponds to marginal likelihood values in `theta_range`. Find the $\\theta$ value that gives the maximum value in this list and provide your final estimate of most likely $\\theta$. **<span style=\"color:blue\">(5 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code goes here\n",
    "\n",
    "# print your theta estimate, e.g.:\n",
    "# print(theta_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Posterior sampling (35 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will perform posterior sampling for the following model\n",
    "\\begin{align*}\n",
    "    p(x) &\\propto \\exp(-x_1^2/10 - x_2^2/10 - 2 (x_2 - x_1^2)^2), \\\\\n",
    "    p(y | x) &= \\mathcal{N}(y; H x, 0.1)\n",
    "\\end{align*}\n",
    "where $H = [0, 1]$. In this exercise, we assume that we have observed $y = 2$ and would like to implement a few sampling methods.\n",
    "\n",
    "Before starting this exercise, please try to understand how the posterior density should look like. The discussion we had during the lecture about Exercise 6.2 (see Panopto if you have not attended) should help you here to understand the posterior density. Note though quantities and various details are **different** here. You should have a good idea about the posterior density before starting this exercise to be able to set the hyperparameters such as the chain-length, proposal noise, and the step-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([2.0])\n",
    "sig_lik = 0.1\n",
    "H = np.array([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** In what follows, you will have to code the log-prior and log-likelihood functions. Do **not** use any library, code the log densities directly. **<span style=\"color:blue\">(5 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(): # code banana density for visualisation purposes\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "def log_prior(): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "def log_likelihood(): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "# uncomment below and evaluate your prior and likelihood (do not remove)\n",
    "# print(log_prior([0, 1]))\n",
    "# print(log_likelihood(y, np.array([0, 1]), sig_lik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii)** Next, implement **the random walk Metropolis algorithm (RWMH)** for this target. Set an appropriate chain length, proposal variance, and `burnin` value. Plot a scatter-plot with your samples (see the visualisation function below). Use log-densities only. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your code here\n",
    "\n",
    "# uncomment and plot your results (do not remove)\n",
    "# x_bb = np.linspace(-4, 4, 100)\n",
    "# y_bb = np.linspace(-2, 6, 100)\n",
    "# X_bb , Y_bb = np.meshgrid(x_bb , y_bb)\n",
    "# Z_bb = np.zeros((100 , 100))\n",
    "# for i in range(100):\n",
    "#     for j in range(100):\n",
    "#         Z_bb[i, j] = prior([X_bb[i, j], Y_bb[i, j]])\n",
    "# plt.contourf(X_bb , Y_bb , Z_bb , 100 , cmap='RdBu')\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# plt.show()\n",
    "\n",
    "# Note that above x vector (your samples) is assumed to be (N, 2).\n",
    "# It does not have to be this way (You can change the name of the variable x too).\n",
    "# i.e., If your x vector is (2, N), then use\n",
    "# plt.scatter(x[0, :], x[1, :], s=10 , c='white')\n",
    "# instead of\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(iii)** Now implement **Metropolis-adjusted Langevin algorithm**. For this, you will need to code the gradient of the density and use it in the proposal as described in the lecture notes. Set an appropriate chain length, step-size, and `burnin` value. Plot a scatter-plot with your samples (see the visualisation function below). Use log-densities only. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_prior(): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "def grad_log_likelihood(): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "def log_MALA_kernel(): # fill in the arguments\n",
    "    # fill in the function\n",
    "    pass # remove this line\n",
    "    return # fill in the return value\n",
    "\n",
    "gam = 0.01\n",
    "\n",
    "# fill in your code here\n",
    "\n",
    "# uncomment and plot your results (do not remove)\n",
    "# x_bb = np.linspace(-4, 4, 100)\n",
    "# y_bb = np.linspace(-2, 6, 100)\n",
    "# X_bb , Y_bb = np.meshgrid(x_bb , y_bb)\n",
    "# Z_bb = np.zeros((100 , 100))\n",
    "# for i in range(100):\n",
    "#     for j in range(100):\n",
    "#         Z_bb[i, j] = prior([X_bb[i, j], Y_bb[i, j]])\n",
    "# plt.contourf(X_bb , Y_bb , Z_bb , 100 , cmap='RdBu')\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# plt.show()\n",
    "\n",
    "# Note that above x vector (your samples) is assumed to be (N, 2).\n",
    "# It does not have to be this way (You can change the name of the variable x too).\n",
    "# i.e., If your x vector is (2, N), then use\n",
    "# plt.scatter(x[0, :], x[1, :], s=10 , c='white')\n",
    "# instead of\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(iv)** Next, implement **unadjusted Langevin algorithm**. For this, you will need to code the gradient of the density and use it in the proposal as described in the lecture notes. Set an appropriate chain length, step-size, and `burnin` value. Plot a scatter-plot with your samples (see the visualisation function below). Use log-densities only. **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your code here\n",
    "\n",
    "# uncomment and plot your results (do not remove)\n",
    "# x_bb = np.linspace(-4, 4, 100)\n",
    "# y_bb = np.linspace(-2, 6, 100)\n",
    "# X_bb , Y_bb = np.meshgrid(x_bb , y_bb)\n",
    "# Z_bb = np.zeros((100 , 100))\n",
    "# for i in range(100):\n",
    "#     for j in range(100):\n",
    "#         Z_bb[i, j] = prior([X_bb[i, j], Y_bb[i, j]])\n",
    "# plt.contourf(X_bb , Y_bb , Z_bb , 100 , cmap='RdBu')\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# plt.show()\n",
    "\n",
    "# Note that above x vector (your samples) is assumed to be (N, 2).\n",
    "# It does not have to be this way (You can change the name of the variable x too).\n",
    "# i.e., If your x vector is (2, N), then use\n",
    "# plt.scatter(x[0, :], x[1, :], s=10 , c='white')\n",
    "# instead of\n",
    "# plt.scatter(x[:, 0], x[:, 1], s=10 , c='white')\n",
    "# in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Gibbs sampling for 2D posterior (25 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will first derive a Gibbs sampler by deriving full conditionals. Then we will describe a method to estimate marginal likelihoods using Gibbs output (and you will be asked to implement the said method given the description).\n",
    "\n",
    "Consider the following probabilistic model\n",
    "\\begin{align*}\n",
    "p(x_1) &= \\mathcal{N}(x_1; \\mu_1, \\sigma_1^2), \\\\\n",
    "p(x_2) &= \\mathcal{N}(x_2; \\mu_2, \\sigma_2^2), \\\\\n",
    "p(y | x_1, x_2) &= \\mathcal{N}(y; x_1 + x_2, \\sigma_y^2),\n",
    "\\end{align*}\n",
    "where $y$ is a scalar observation and $x_1, x_2$ are latent variables. This is a simple model where we observe a sum of two random variables and want to construct possible values of $x_1, x_2$ given the observation $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** Derive the Gibbs sampler for this model, by deriving full conditionals $p(x_1 | x_2, y)$ and $p(x_2 | x_1, y)$ (You can use Example 3.2 but note that this case is different). **<span style=\"color:blue\">(10 marks)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** <span style=\"color:red\">**Your answer here in LaTeX. If you prefer handwritten, please write here \"Handwritten\" and attach your pdf to the end and clearly number your answer Q3(i)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii)** Let us set $y = 5$, $\\mu_1 = 0$, $\\mu_2 = 0$, $\\sigma_1 = 0.1$, $\\sigma_2 = 0.1$, and $\\sigma_y = 0.01$.\n",
    "\n",
    "Implement the Gibbs sampler you derived in Part (i). Set an appropriate chain length and `burnin` value. Plot a scatter plot of your samples (see the visualisation function below). Discuss the result: Why does the posterior look like this? **<span style=\"color:blue\">(15 marks)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 5\n",
    "\n",
    "mu1 = 0\n",
    "mu2 = 0\n",
    "sig1 = 0.1\n",
    "sig2 = 0.1\n",
    "\n",
    "sig_y = 0.01\n",
    "\n",
    "# fill in your code here for Gibbs sampling\n",
    "\n",
    "# uncomment and plot your results (do not remove)\n",
    "# plt.scatter(x1_chain, x2_chain, s=1)\n",
    "# plt.xlabel(\"x1\")\n",
    "# plt.ylabel(\"x2\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Your discussion goes here. (in words).**</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
